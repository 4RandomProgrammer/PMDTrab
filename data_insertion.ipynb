{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter Strike Global Offensive Match Result Prediction\n",
    "- **Leonardo Valerio Morales 771030**\n",
    "- **Luis Felipe Dobner Henriques 771036**\n",
    "\n",
    "This notebook executes data pre-processing and predictive analysis of Counter Strike Global Offensive Matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Variables\n",
    "This step loads Everything needed for Neo4j and Apache Kudu to Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviroment Variables\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "KUDU_MASTER = 'kudu-master-1:7051'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviroment Variables\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.kudu:kudu-spark3_2.12:1.13.0.7.1.5.17-1,org.neo4j:neo4j-connector-apache-spark_2.12:5.0.1_for_spark_3 --repositories https://repository.cloudera.com/artifactory/cloudera-repos/ pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviroment Variables\n",
    "spark = SparkSession.builder.config('spark.packages', 'org.apache.kudu:kudu-spark3_2.12:1.13.0.7.1.5.17-1,org.neo4j:neo4j-connector-apache-spark_2.12:5.0.1_for_spark_3').getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel('OFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_kudu(df, table):\n",
    "    df.write.option('kudu.master', KUDU_MASTER).option('kudu.table', f'impala::default.{table}').mode('append').format('kudu').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_kudu(table):\n",
    "     return spark.read.option('kudu.master', KUDU_MASTER).option('kudu.table', f'impala::default.{table}').format('kudu').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "This step will create the tables and schemas in both databases and load the concerning data from the dataset into Neo4j and Apache Kudu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./archive.zip\n",
      "  inflating: economy.csv             \n",
      "  inflating: picks.csv               \n",
      "  inflating: players.csv             \n",
      "  inflating: results.csv             \n"
     ]
    }
   ],
   "source": [
    "!unzip -n {'./archive.zip'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- team_1: string (nullable = true)\n",
      " |-- team_2: string (nullable = true)\n",
      " |-- _map: string (nullable = true)\n",
      " |-- result_1: string (nullable = true)\n",
      " |-- result_2: string (nullable = true)\n",
      " |-- map_winner: string (nullable = true)\n",
      " |-- starting_ct: string (nullable = true)\n",
      " |-- ct_1: string (nullable = true)\n",
      " |-- t_2: string (nullable = true)\n",
      " |-- t_1: string (nullable = true)\n",
      " |-- ct_2: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- match_id: string (nullable = true)\n",
      " |-- rank_1: string (nullable = true)\n",
      " |-- rank_2: string (nullable = true)\n",
      " |-- map_wins_1: string (nullable = true)\n",
      " |-- map_wins_2: string (nullable = true)\n",
      " |-- match_winner: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Extraction\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./results.csv\")\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, match_id: string, event_id: string, team_1: string, team_2: string, best_of: string, _map: string, t1_start: string, t2_start: string, 1_t1: string, 2_t1: string, 3_t1: string, 4_t1: string, 5_t1: string, 6_t1: string, 7_t1: string, 8_t1: string, 9_t1: string, 10_t1: string, 11_t1: string, 12_t1: string, 13_t1: string, 14_t1: string, 15_t1: string, 16_t1: string, 17_t1: string, 18_t1: string, 19_t1: string, 20_t1: string, 21_t1: string, 22_t1: string, 23_t1: string, 24_t1: string, 25_t1: string, 26_t1: string, 27_t1: string, 28_t1: string, 29_t1: string, 30_t1: string, 1_t2: string, 2_t2: string, 3_t2: string, 4_t2: string, 5_t2: string, 6_t2: string, 7_t2: string, 8_t2: string, 9_t2: string, 10_t2: string, 11_t2: string, 12_t2: string, 13_t2: string, 14_t2: string, 15_t2: string, 16_t2: string, 17_t2: string, 18_t2: string, 19_t2: string, 20_t2: string, 21_t2: string, 22_t2: string, 23_t2: string, 24_t2: string, 25_t2: string, 26_t2: string, 27_t2: string, 28_t2: string, 29_t2: string, 30_t2: string, 1_winner: string, 2_winner: string, 3_winner: string, 4_winner: string, 5_winner: string, 6_winner: string, 7_winner: string, 8_winner: string, 9_winner: string, 10_winner: string, 11_winner: string, 12_winner: string, 13_winner: string, 14_winner: string, 15_winner: string, 16_winner: string, 17_winner: string, 18_winner: string, 19_winner: string, 20_winner: string, 21_winner: string, 22_winner: string, 23_winner: string, 24_winner: string, 25_winner: string, 26_winner: string, 27_winner: string, 28_winner: string, 29_winner: string, 30_winner: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Extraction\n",
    "df2 = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".load(\"./economy.csv\")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[match_id: string, _map: string, team_1: string, team_2: string, map_winner: string, starting_ct: string, match_winner: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_limited = df1.select(df1['match_id'], df1['_map'], df1['team_1'], df1['team_2'], df1['map_winner'], df1['starting_ct'], df1['match_winner'])\n",
    "display(df_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[match_id: string, _map: string, team_1: string, team_2: string, best_of: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2_limited = df2.select(df2['match_id'], df2['_map'], df2['team_1'], df2['team_2'], df2['best_of'])\n",
    "display(df2_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[partida: string, mapa: string, equipe1: string, equipe2: string, vitorioso: string, ct: string, tr: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PrÃ©-processamento para Tabela de Jogos no Kudu\n",
    "df_collect = df_limited.collect()\n",
    "returnval = []\n",
    "for i in range(df_limited.count()):\n",
    "    selected_row = df_collect[i]\n",
    "\n",
    "    map_winner_num = selected_row['map_winner']\n",
    "    map_winner = selected_row[f'team_{map_winner_num}']\n",
    "\n",
    "    starting_ct_num = selected_row['starting_ct']\n",
    "    start_ct = selected_row[f'team_{starting_ct_num}']\n",
    "\n",
    "    tr = ''\n",
    "    if start_ct == selected_row['team_2']:\n",
    "        tr = selected_row['team_1']\n",
    "    else:\n",
    "        tr = selected_row['team_2']\n",
    "        \n",
    "    \n",
    "    returnval.append([selected_row['match_id'], selected_row['_map'], selected_row['team_1'], selected_row['team_2'], map_winner, start_ct, tr])\n",
    "    \n",
    "\n",
    "schema = ['partida','mapa', 'equipe1','equipe2','vitorioso','ct','tr']\n",
    "df_mapas = spark.createDataFrame(returnval, schema)\n",
    "display(df_mapas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_in_kudu(df_mapas,'jogos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[partida: string, mapa: string, equipe1: string, equipe2: string, vitorioso: string, ct: string, tr: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = read_from_kudu('jogos')\n",
    "table.createOrReplaceTempView('jogos')\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[match_id: string, _map: string, team_1: string, team_2: string, match_winner: string, best_of: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "condition = [df_limited.match_id == df2_limited.match_id, df_limited._map == df2_limited._map]\n",
    "df_join = df_limited.join(df2_limited,condition,\"inner\").select(df_limited.match_id,df_limited._map, df_limited.team_1, df_limited.team_2,df_limited.match_winner,df2_limited.best_of)\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[partida: string, mapa: string, equipe1: string, equipe2: string, vitorioso: string, md: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_join = df_join.withColumnRenamed('match_id', 'partida') \\\n",
    "                    .withColumnRenamed('_map', 'mapa') \\\n",
    "                .withColumnRenamed('team_1', 'equipe1') \\\n",
    "                .withColumnRenamed('team_2', 'equipe2') \\\n",
    "                .withColumnRenamed('match_winner', 'vitorioso') \\\n",
    "                .withColumnRenamed('best_of', 'md')\n",
    "df_join = df_join.limit(1000)\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|equipe2|\n",
      "+-------+\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|equipe1|\n",
      "+-------+\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_missing_teams = df_join.select('equipe2').distinct().subtract(df_join.select('equipe1').distinct())\n",
    "df_teams = df_join.select('equipe1').distinct().union(df_missing_teams).withColumnRenamed('equipe1','teams')\n",
    "df_join.select('equipe2').distinct().exceptAll(df_teams).show()\n",
    "df_join.select('equipe1').distinct().exceptAll(df_teams).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_in_kudu(df_join,'proc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[partida: string, mapa: string, equipe1: string, equipe2: string, vitorioso: string, md: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teste = read_from_kudu('proc')\n",
    "teste.createOrReplaceTempView('proc')\n",
    "display(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[equipe: string, jogos: bigint, vitorias: bigint, derrotas: bigint, md1: bigint, md2: bigint, md3: bigint, md5: bigint, jmd1: bigint, jmd2: bigint, jmd3: bigint, jmd5: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_collect = df_teams.collect()\n",
    "data = []\n",
    "for i in range(df_teams.count()):\n",
    "    selected_row = df_collect[i]\n",
    "    current_team = selected_row['teams']\n",
    "    print(current_team)\n",
    "    \n",
    "    num_jogos1 = spark.sql(f'SELECT COUNT(*) as cnt FROM (SELECT DISTINCT partida FROM proc WHERE equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\") a')\n",
    "    total_jogos = num_jogos1.collect()[0][0]\n",
    "\n",
    "    vitorias = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE (equipe1 = \"{current_team}\" and vitorioso = \"1\") or (equipe2 = \"{current_team}\" and vitorioso = \"2\" ) ) a').collect()[0][0] \n",
    "    derrotas = total_jogos - vitorias\n",
    "\n",
    "    jmd5 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"5\" and (equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\")) a').collect()[0][0]\n",
    "    md5 = 0\n",
    "    if jmd5 > 0:\n",
    "        md5 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"5\" and ((equipe1 = \"{current_team}\"  and vitorioso = \"1\")  or (equipe2 = \"{current_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "\n",
    "    jmd3 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"3\" and (equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\")) a').collect()[0][0]\n",
    "    md3 = 0\n",
    "    if jmd3 > 0:\n",
    "        md3 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"3\" and ((equipe1 = \"{current_team}\" and vitorioso = \"1\") or (equipe2 = \"{current_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "\n",
    "    jmd2 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"2\" and (equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\")) a').collect()[0][0]\n",
    "    md2 = 0\n",
    "    if jmd2 > 0:\n",
    "        md2 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"2\" and ((equipe1 = \"{current_team}\" and vitorioso = \"1\") or (equipe2 = \"{current_team}\" and vitorioso = \"2\") ) ) a').collect()[0][0]\n",
    "\n",
    "    jmd1 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"1\" and (equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\")) a').collect()[0][0]\n",
    "    md1 = 0\n",
    "    if jmd1 > 0:    \n",
    "        md1 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"1\" and  ((equipe1 = \"{current_team}\"  and vitorioso = \"1\") or (equipe2 = \"{current_team}\" and vitorioso = \"2\"))) a').collect()[0][0]\n",
    "        \n",
    "\n",
    "    data.append([current_team,total_jogos,vitorias,derrotas, md1, md2, md3, md5,jmd1, jmd2, jmd3,jmd5])\n",
    "\n",
    "schema = ['equipe','jogos', 'vitorias', 'derrotas','md1', 'md2', 'md3', 'md5','jmd1', 'jmd2', 'jmd3','jmd5']\n",
    "df_equipes = spark.createDataFrame(data, schema)\n",
    "display(df_equipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- equipe: string (nullable = true)\n",
      " |-- jogos: decimal(8,5) (nullable = true)\n",
      " |-- vitorias: decimal(8,5) (nullable = true)\n",
      " |-- derrotas: decimal(8,5) (nullable = true)\n",
      " |-- md1: decimal(8,5) (nullable = true)\n",
      " |-- md2: decimal(8,5) (nullable = true)\n",
      " |-- md3: decimal(8,5) (nullable = true)\n",
      " |-- md5: decimal(8,5) (nullable = true)\n",
      " |-- jmd1: decimal(8,5) (nullable = true)\n",
      " |-- jmd2: decimal(8,5) (nullable = true)\n",
      " |-- jmd3: decimal(8,5) (nullable = true)\n",
      " |-- jmd5: decimal(8,5) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_equipes = df_equipes.withColumn('jogos', df_equipes.jogos.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('vitorias', df_equipes.vitorias.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('derrotas', df_equipes.derrotas.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('md1', df_equipes.md1.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('md2', df_equipes.md2.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('md3', df_equipes.md3.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('md5', df_equipes.md5.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('jmd1', df_equipes.jmd1.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('jmd2', df_equipes.jmd2.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('jmd3', df_equipes.jmd3.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('jmd5', df_equipes.jmd5.cast(DecimalType(8, 5)))\n",
    "df_equipes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_in_kudu(df_equipes,'equipes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipes = read_from_kudu('equipes')\n",
    "equipes.createOrReplaceTempView('equipes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- equipe: string (nullable = false)\n",
      " |-- jogos: decimal(8,5) (nullable = true)\n",
      " |-- vitorias: decimal(8,5) (nullable = true)\n",
      " |-- derrotas: decimal(8,5) (nullable = true)\n",
      " |-- md1: decimal(8,5) (nullable = true)\n",
      " |-- md2: decimal(8,5) (nullable = true)\n",
      " |-- md3: decimal(8,5) (nullable = true)\n",
      " |-- md5: decimal(8,5) (nullable = true)\n",
      " |-- jmd1: decimal(8,5) (nullable = true)\n",
      " |-- jmd2: decimal(8,5) (nullable = true)\n",
      " |-- jmd3: decimal(8,5) (nullable = true)\n",
      " |-- jmd5: decimal(8,5) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equipes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j Data Pre Processing\n",
    "This step will use the loaded data in Apache Kudu to pre process specific team win rates, and insert that data into de Neo4j database for later use during result prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating all the nodes\n",
    "team_node_schema = StructType([StructField(\"name\", StringType(), False),\\\n",
    "                     StructField(\"mostPicked\", StringType(), True),\\\n",
    "                     StructField(\"mostBanned\", StringType(), True),\\\n",
    "                     StructField(\"mostWon\", StringType(), True),\\\n",
    "                     StructField(\"mostLost\", StringType(), True)])\n",
    "\n",
    "list_team_name = spark.sql(\"SELECT DISTINCT equipe FROM equipes\")\n",
    "\n",
    "rows = [Row(name=row[\"equipe\"], mostPicked=None,mostBanned=None,mostWon=None,mostLost=None) for row in list_team_name.collect()]\n",
    "\n",
    "df = spark.createDataFrame(rows,schema=team_node_schema)\n",
    "df.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    " .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    " .option(\"authentication.type\", \"none\")\\\n",
    " .option(\"labels\", \":Team\")\\\n",
    " .mode(\"Append\")\\\n",
    " .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating relationships\n",
    "list_map_name = spark.sql(\"SELECT DISTINCT mapa FROM proc\")\n",
    "df_collect = df_teams.collect()\n",
    "for x in range(df_teams.count()):\n",
    "    origin_row = df_collect[x]\n",
    "    origin_team = origin_row['teams']   \n",
    "    for i in range(df_teams.count()):\n",
    "        target_row = df_collect[i]\n",
    "        target_team = target_row['teams']\n",
    "        \n",
    "        if origin_team == target_team:\n",
    "            continue\n",
    "        jogos = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE (equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\") or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\") ) a').collect()[0][0]\n",
    "        \n",
    "        if jogos == 0:\n",
    "            continue\n",
    "            \n",
    "\n",
    "        vits = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE (equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\"  and vitorioso = \"1\")  or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\" and vitorioso = \"2\") )  a').collect()[0][0]\n",
    "\n",
    "        jmd5 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"5\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\") or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\"))) a').collect()[0][0]\n",
    "        if jmd5 > 0:    \n",
    "            md5 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"5\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\"  and vitorioso = \"1\")  or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "    \n",
    "        jmd3 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"3\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\") or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\"))) a').collect()[0][0]\n",
    "        if jmd3 > 0:\n",
    "            md3 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"3\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\"  and vitorioso = \"1\")  or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "    \n",
    "        jmd2 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"2\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\") or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\"))) a').collect()[0][0]\n",
    "        if jmd2 > 0:\n",
    "            md2 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"2\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\"  and vitorioso = \"1\")  or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "    \n",
    "        jmd1 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"1\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\") or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\")) a').collect()[0][0]\n",
    "        if jmd1 > 0:    \n",
    "            md1 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"1\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\"  and vitorioso = \"1\")  or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "\n",
    "        wr = vits / jogos\n",
    "        \n",
    "        txmd5 = 0\n",
    "        if jmd5 != 0:\n",
    "            txmd5 = md5/jmd5\n",
    "        txmd3 = 0\n",
    "        if jmd3 != 0:\n",
    "            txmd3 = md3/jmd3\n",
    "        txmd2 = 0\n",
    "        if jmd2 != 0:\n",
    "            txmd2 = md2/jmd2\n",
    "        txmd1 = 0\n",
    "        if jmd1 != 0:\n",
    "            txmd5 = md1/jmd1\n",
    "\n",
    "        \n",
    "        \n",
    "        relationship_type = 'played'\n",
    "        relationship_df = spark.createDataFrame(\n",
    "             [(origin_team, target_team, relationship_type,wr, txmd5,txmd3,txmd2,txmd1)],\n",
    "             ['src', 'dst', 'relationship_type', 'winrate','bo5','bo3','bo2','bo1']\n",
    "         )\n",
    "        relationship_df.write \\\n",
    "            .format('org.neo4j.spark.DataSource') \\\n",
    "            .option('url', 'bolt://neo4j:7687') \\\n",
    "            .option(\"authentication.type\", \"none\")\\\n",
    "            .option('relationship.save.strategy', 'keys') \\\n",
    "            .option(\"relationship.source.node.keys\", \"src:name\") \\\n",
    "            .option('relationship.source.labels', ':Team') \\\n",
    "            .option(\"relationship.target.node.keys\", \"dst:name\") \\\n",
    "            .option('relationship.target.labels', ':Team') \\\n",
    "            .option('relationship', relationship_type) \\\n",
    "            .option('relationship.properties', \"winrate:winrate,bo5:bo5,bo3:bo3,bo2:bo2,bo1:bo1\") \\\n",
    "            .mode('append') \\\n",
    "            .save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Prediction\n",
    "This is the main step to be used in result prediction, it will load data from Neo4j, while simultaneously processing raw generic data present in Apache Kudu. Results from both databases will then be fed into an algorithm that predicts the winner of the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalMapWR(team, map):\n",
    "    games = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE mapa = \"{map}\" and (equipe1 = \"{team}\" or equipe2 = \"{team}\")) a').collect()[0][0]\n",
    "    if games == 0:\n",
    "            return 0\n",
    "    wins = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and mapa = \"{map}\" and (equipe1 = \"{team}\" or equipe2 = \"{team}\")) a').collect()[0][0]\n",
    "    return float(wins/games)\n",
    "\n",
    "def generalMapSideWR(team, map, side):\n",
    "    if side == 'TR': \n",
    "        games = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE mapa = \"{map}\" and tr = \"{team}\") a').collect()[0][0]\n",
    "        if games == 0:\n",
    "                return 0\n",
    "        wins = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and mapa = \"{map}\" and tr = \"{team}\") a').collect()[0][0]\n",
    "        return float(wins/games)\n",
    "    if side == 'CT':\n",
    "        games = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE mapa = \"{map}\" and ct = \"{team}\") a').collect()[0][0]\n",
    "        if games == 0:\n",
    "                return 0\n",
    "        wins = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and mapa = \"{map}\" and ct = \"{team}\" ) a').collect()[0][0]\n",
    "        return float(wins/games)\n",
    "    return 0\n",
    "\n",
    "def generalSideWR(team, side):\n",
    "    if side == 'TR':\n",
    "        games = spark.sql(f'SELECT COUNT(partida) as jogos FROM (SELECT partida, mapa FROM jogos WHERE tr = \"{team}\") a').collect()[0][0]\n",
    "        if games == 0:\n",
    "            return 0\n",
    "        wins = spark.sql(f'SELECT COUNT(partida) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and tr = \"{team}\") a').collect()[0][0]\n",
    "        return float(wins/games)\n",
    "    if side == 'CT':\n",
    "        games = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida, mapa FROM jogos WHERE ct = \"{team}\") a').collect()[0][0]\n",
    "        if games == 0:\n",
    "            return 0\n",
    "        wins = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and ct = \"{team}\") a').collect()[0][0]\n",
    "        return float(wins/games)\n",
    "    return 0\n",
    "\n",
    "def generalTeamWR(team):\n",
    "    games = spark.sql(f'SELECT jogos FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return 0\n",
    "    wins = spark.sql(f'SELECT vitorias FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return float(wins/games)\n",
    "\n",
    "def generalBO1WR(team):\n",
    "    games = spark.sql(f'SELECT jmd1 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return 0\n",
    "    wins = spark.sql(f'SELECT md1 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return float(wins/games)\n",
    "\n",
    "def generalBO2WR(team):\n",
    "    games = spark.sql(f'SELECT jmd2 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return 0\n",
    "    wins = spark.sql(f'SELECT md2 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return float(wins/games)\n",
    "\n",
    "def generalBO3WR(team):\n",
    "    games = spark.sql(f'SELECT jmd3 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return 0\n",
    "    wins = spark.sql(f'SELECT md3 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return float(wins/games)\n",
    "\n",
    "def generalBO5WR(team):\n",
    "    games = spark.sql(f'SELECT jmd5 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return 0\n",
    "    wins = spark.sql(f'SELECT md5 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return float(wins/games)\n",
    "\n",
    "def specificTeamToTeamWR(team1, team2):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = '{team1}' AND n2.name = '{team2}' RETURN r.winrate\")\\\n",
    "        .load()\n",
    "    if df.isEmpty():\n",
    "        return 0\n",
    "    return float(df.collect()[0][0])\n",
    "\n",
    "def specificTeamToTeamBO1WR(team1, team2):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = '{team1}' AND n2.name = '{team2}' RETURN r.bo1\")\\\n",
    "        .load()\n",
    "    if df.isEmpty():\n",
    "        return 0\n",
    "    return float(df.collect()[0][0])\n",
    "\n",
    "def specificTeamToTeamBO2WR(team1, team2):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = '{team1}' AND n2.name = '{team2}' RETURN r.bo2\")\\\n",
    "        .load()\n",
    "    if df.isEmpty():\n",
    "        return 0\n",
    "    return float(df.collect()[0][0])\n",
    "\n",
    "def specificTeamToTeamBO3WR(team1, team2):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = '{team1}' AND n2.name = '{team2}' RETURN r.bo3\")\\\n",
    "        .load()\n",
    "    if df.isEmpty():\n",
    "        return 0\n",
    "    return float(df.collect()[0][0])\n",
    "\n",
    "def specificTeamToTeamBO5WR(team1, team2):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = '{team1}' AND n2.name = '{team2}' RETURN r.bo5\")\\\n",
    "        .load()\n",
    "    if df.isEmpty():\n",
    "        return 0\n",
    "    return float(df.collect()[0][0])\n",
    "\n",
    "def predict(team1 = None, side1 = None, team2 = None, map = None, bo=None):\n",
    "    wr = 0\n",
    "    side_wr = 0\n",
    "    map_wr = 0\n",
    "    map_side_wr = 0\n",
    "    team_to_team_wr = 0\n",
    "    team_to_team_side_wr = 0\n",
    "    bo_wr = 0\n",
    "    team_to_team_bo_wr = 0\n",
    "    if team1 == None:\n",
    "        return \"Pelo menos um time deve ser fornecido\"\n",
    "    wr = generalTeamWR(team1);\n",
    "    if side1 != None:\n",
    "        side_wr = generalSideWR(team1,side1)\n",
    "    if map != None:\n",
    "        map_wr = generalMapWR(team1,map)\n",
    "    if bo != None:\n",
    "        match bo:\n",
    "            case \"md1\":\n",
    "                bo_wr = generalBO1WR(team1)\n",
    "            case \"md2\":\n",
    "                bo_wr = generalBO2WR(team1)\n",
    "            case \"md3\":\n",
    "                bo_wr = generalBO3WR(team1)\n",
    "            case \"md5\":\n",
    "                bo_wr = generalBO5WR(team1)\n",
    "            case _:\n",
    "                bo_wr = 0\n",
    "    if side1 != None and map != None:\n",
    "        map_side_wr = generalMapSideWR(team1,map,side1)\n",
    "    if team2 != None:\n",
    "        team_to_team_wr = specificTeamToTeamWR(team1,team2)\n",
    "    if team2 != None and bo != None:\n",
    "        match bo:\n",
    "            case \"md1\":\n",
    "                team_to_team_bo_wr = specificTeamToTeamBO1WR(team1, team2)\n",
    "            case \"md2\":\n",
    "                team_to_team_bo_wr = specificTeamToTeamBO2WR(team1, team2)\n",
    "            case \"md3\":\n",
    "                team_to_team_bo_wr = specificTeamToTeamBO3WR(team1, team2)\n",
    "            case \"md5\":\n",
    "                team_to_team_bo_wr = specificTeamToTeamBO5WR(team1, team2)\n",
    "            case _:\n",
    "                team_to_team_bo_wr = 0\n",
    "\n",
    "    return 0.05*wr+0.05*bo_wr+0.05*side_wr+0.1*team_to_team_wr+0.1*team_to_team_bo_wr+0.15*team_to_team_side_wr+0.25*map_wr+0.25*map_side_wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n",
      "0.375\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# equipes.show()\n",
    "print(generalBO1WR(\"RED Canids\"))\n",
    "print(generalBO2WR(\"RED Canids\"))\n",
    "print(generalBO3WR(\"MAD Lions\"))\n",
    "print(generalTeamWR(\"Complexity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# equipes.show()\n",
    "print(specificTeamToTeamBO2WR(\"Keyd\",\"RED Canids\"))\n",
    "print(specificTeamToTeamBO3WR(\"G2\",\"Liquid\"))\n",
    "print(specificTeamToTeamBO5WR(\"Evidence\",\"2K\"))\n",
    "print(specificTeamToTeamBO5WR(\"Evidence\",\"2K\"))\n",
    "print(specificTeamToTeamWR(\"Natus Vincere\",\"Vitality\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "1.0\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(generalMapWR(\"RED Canids\", \"Vertigo\"))\n",
    "print(generalMapSideWR(\"RED Canids\", \"Vertigo\",\"TR\"))\n",
    "print(generalMapSideWR(\"RED Canids\", \"Vertigo\",\"CT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19514285714285712\n",
      "0.5019607843137255\n",
      "0.0644607843137255\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"fnatic\",\"TR\",\"RED Canids\",\"Vertigo\",\"md3\"))\n",
    "print(predict(\"RED Canids\",\"TR\",\"fnatic\",\"Vertigo\",\"md3\"))\n",
    "print(predict(\"RED Canids\",\"TR\",\"fnatic\",bo=\"md3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Queries\n",
    "This section is used to call other queries not linked to result prediction like objective queries or player statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low priority TODO \n",
    "# Define functions to execute objective queries in Neo4j\n",
    "# Define functions to execute generic statistics queries in Apache Kudu\n",
    "# Define interface to call above functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
