{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Test\n",
    "This notebook is intended to be used to test the database environment, specifically the connection of the spark environment with the neo4j and apache kudu databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "KUDU_MASTER = 'kudu-master-1:7051'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.kudu:kudu-spark3_2.12:1.13.0.7.1.5.17-1,org.neo4j:neo4j-connector-apache-spark_2.12:5.0.1_for_spark_3 --repositories https://repository.cloudera.com/artifactory/cloudera-repos/ pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config('spark.packages', 'org.apache.kudu:kudu-spark3_2.12:1.13.0.7.1.5.17-1,org.neo4j:neo4j-connector-apache-spark_2.12:5.0.1_for_spark_3').getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel('OFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kudu Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial read from Kudu\n",
    "Reading from tableA, that was created during the execution of the pre test commands.\n",
    "Should return an empty dataframe with the schema specified during the creation of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[literal: string, nota: decimal(8,5), dados: decimal(8,5), idade: int, lorem: int, ipsum: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+-----+-----+\n",
      "|literal|nota|dados|idade|lorem|ipsum|\n",
      "+-------+----+-----+-----+-----+-----+\n",
      "+-------+----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = spark.read.option('kudu.master', KUDU_MASTER).option('kudu.table', f'impala::default.testA').format('kudu').load()\n",
    "table.createOrReplaceTempView('testA')\n",
    "display(table)\n",
    "table.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion of new rows\n",
    "The Dataframe df_kudu is created with column information and two rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kudu = spark.createDataFrame(\n",
    " [(\"conseguimos?\", 3.2, 2.0, 1000, 300, \"sad\"),(\"teste\", 22.5,4.75, -500, -1000, \"Jarles\")],\n",
    " [\"literal\",\"nota\", \"dados\",\"idade\",\"lorem\",\"ipsum\"]\n",
    ")\n",
    "df_kudu = df_kudu.withColumn('nota', df_kudu.nota.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('dados', df_kudu.dados.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('idade', df_kudu.idade.cast(IntegerType())) \\\n",
    "    .withColumn('lorem', df_kudu.lorem.cast(IntegerType()))\n",
    "df_kudu.write.option('kudu.master', KUDU_MASTER).option('kudu.table', f'impala::default.testA').mode('append').format('kudu').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Kudu again\n",
    "Performing the same read operation as the initial step, however, as new data was inserted it is expected to return the same schema but with two new rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[literal: string, nota: decimal(8,5), dados: decimal(8,5), idade: int, lorem: int, ipsum: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------+-----+-----+------+\n",
      "|literal     |nota    |dados  |idade|lorem|ipsum |\n",
      "+------------+--------+-------+-----+-----+------+\n",
      "|conseguimos?|3.20000 |2.00000|1000 |300  |sad   |\n",
      "|teste       |22.50000|4.75000|-500 |-1000|Jarles|\n",
      "+------------+--------+-------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = spark.read.option('kudu.master', KUDU_MASTER).option('kudu.table', f'impala::default.testA').format('kudu').load()\n",
    "table.createOrReplaceTempView('testA')\n",
    "display(table)\n",
    "table.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial read from neo4j\n",
    "In this step we try to query from neo4j all the nodes of label \"Person\", it should return an empty Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[<id>: bigint, <labels>: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|<id>|<labels>|\n",
      "+----+--------+\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    " .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    " .option(\"authentication.type\", \"none\")\\\n",
    " .option(\"labels\", \"Person\")\\\n",
    " .load()\n",
    "display(df)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing into neo4j\n",
    "In this step we create a new dataframe with two new rows, or in this case nodes. We specify the properties names too and write them with the label \"Person\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    " [(3, \"Carlos\"),(4, \"Jarles\")],\n",
    " [\"id\", \"name\"]\n",
    ")\n",
    "df.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    " .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    " .option(\"authentication.type\", \"none\")\\\n",
    " .option(\"labels\", \":Person\")\\\n",
    " .option(\"node.keys\", \"id\")\\\n",
    " .mode(\"Overwrite\")\\\n",
    " .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading neo4j again\n",
    "This step should return the new nodes inserted in the above cell, as it is the same query used in the initial read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[<id>: bigint, <labels>: array<string>, name: string, id: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+---+\n",
      "|<id>|<labels>|name  |id |\n",
      "+----+--------+------+---+\n",
      "|0   |[Person]|Carlos|3  |\n",
      "|1   |[Person]|Jarles|4  |\n",
      "+----+--------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    " .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    " .option(\"authentication.type\", \"none\")\\\n",
    " .option(\"labels\", \"Person\")\\\n",
    " .load()\n",
    "display(df)\n",
    "df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
