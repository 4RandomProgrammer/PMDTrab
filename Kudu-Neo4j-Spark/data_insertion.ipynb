{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter Strike Global Offensive Match Result Prediction\n",
    "- **Leonardo Valerio Morales 771030**\n",
    "- **Luis Felipe Dobner Henriques 771036**\n",
    "\n",
    "This notebook executes data pre-processing and predictive analysis of Counter Strike Global Offensive Matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Variables\n",
    "This step loads Everything needed for Neo4j and Apache Kudu to Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviroment Variables\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "KUDU_MASTER = 'kudu-master-1:7051'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviroment Variables\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.kudu:kudu-spark3_2.12:1.13.0.7.1.5.17-1,org.neo4j:neo4j-connector-apache-spark_2.12:5.0.1_for_spark_3 --repositories https://repository.cloudera.com/artifactory/cloudera-repos/ pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviroment Variables\n",
    "spark = SparkSession.builder.config('spark.packages', 'org.apache.kudu:kudu-spark3_2.12:1.13.0.7.1.5.17-1,org.neo4j:neo4j-connector-apache-spark_2.12:5.0.1_for_spark_3').getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel('OFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_kudu(df, table):\n",
    "    df.write.option('kudu.master', KUDU_MASTER).option('kudu.table', f'impala::default.{table}').mode('append').format('kudu').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_kudu(table):\n",
    "     return spark.read.option('kudu.master', KUDU_MASTER).option('kudu.table', f'impala::default.{table}').format('kudu').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "This step will create the tables and schemas in both databases and load the concerning data from the dataset into Neo4j and Apache Kudu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./archive.zip\n",
      "  inflating: economy.csv             \n",
      "  inflating: picks.csv               \n",
      "  inflating: players.csv             \n",
      "  inflating: results.csv             \n"
     ]
    }
   ],
   "source": [
    "!unzip -n {'./archive.zip'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- team_1: string (nullable = true)\n",
      " |-- team_2: string (nullable = true)\n",
      " |-- _map: string (nullable = true)\n",
      " |-- result_1: string (nullable = true)\n",
      " |-- result_2: string (nullable = true)\n",
      " |-- map_winner: string (nullable = true)\n",
      " |-- starting_ct: string (nullable = true)\n",
      " |-- ct_1: string (nullable = true)\n",
      " |-- t_2: string (nullable = true)\n",
      " |-- t_1: string (nullable = true)\n",
      " |-- ct_2: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- match_id: string (nullable = true)\n",
      " |-- rank_1: string (nullable = true)\n",
      " |-- rank_2: string (nullable = true)\n",
      " |-- map_wins_1: string (nullable = true)\n",
      " |-- map_wins_2: string (nullable = true)\n",
      " |-- match_winner: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Extraction\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./results.csv\")\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, match_id: string, event_id: string, team_1: string, team_2: string, best_of: string, _map: string, t1_start: string, t2_start: string, 1_t1: string, 2_t1: string, 3_t1: string, 4_t1: string, 5_t1: string, 6_t1: string, 7_t1: string, 8_t1: string, 9_t1: string, 10_t1: string, 11_t1: string, 12_t1: string, 13_t1: string, 14_t1: string, 15_t1: string, 16_t1: string, 17_t1: string, 18_t1: string, 19_t1: string, 20_t1: string, 21_t1: string, 22_t1: string, 23_t1: string, 24_t1: string, 25_t1: string, 26_t1: string, 27_t1: string, 28_t1: string, 29_t1: string, 30_t1: string, 1_t2: string, 2_t2: string, 3_t2: string, 4_t2: string, 5_t2: string, 6_t2: string, 7_t2: string, 8_t2: string, 9_t2: string, 10_t2: string, 11_t2: string, 12_t2: string, 13_t2: string, 14_t2: string, 15_t2: string, 16_t2: string, 17_t2: string, 18_t2: string, 19_t2: string, 20_t2: string, 21_t2: string, 22_t2: string, 23_t2: string, 24_t2: string, 25_t2: string, 26_t2: string, 27_t2: string, 28_t2: string, 29_t2: string, 30_t2: string, 1_winner: string, 2_winner: string, 3_winner: string, 4_winner: string, 5_winner: string, 6_winner: string, 7_winner: string, 8_winner: string, 9_winner: string, 10_winner: string, 11_winner: string, 12_winner: string, 13_winner: string, 14_winner: string, 15_winner: string, 16_winner: string, 17_winner: string, 18_winner: string, 19_winner: string, 20_winner: string, 21_winner: string, 22_winner: string, 23_winner: string, 24_winner: string, 25_winner: string, 26_winner: string, 27_winner: string, 28_winner: string, 29_winner: string, 30_winner: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Extraction\n",
    "df2 = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".load(\"./economy.csv\")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[match_id: string, _map: string, team_1: string, team_2: string, map_winner: string, starting_ct: string, match_winner: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_limited = df1.select(df1['match_id'], df1['_map'], df1['team_1'], df1['team_2'], df1['map_winner'], df1['starting_ct'], df1['match_winner'])\n",
    "display(df_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[match_id: string, _map: string, team_1: string, team_2: string, best_of: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2_limited = df2.select(df2['match_id'], df2['_map'], df2['team_1'], df2['team_2'], df2['best_of'])\n",
    "display(df2_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[partida: string, mapa: string, equipe1: string, equipe2: string, vitorioso: string, ct: string, tr: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PrÃ©-processamento para Tabela de Jogos no Kudu\n",
    "df_collect = df_limited.collect()\n",
    "returnval = []\n",
    "for i in range(df_limited.count()):\n",
    "    selected_row = df_collect[i]\n",
    "\n",
    "    map_winner_num = selected_row['map_winner']\n",
    "    map_winner = selected_row[f'team_{map_winner_num}']\n",
    "\n",
    "    starting_ct_num = selected_row['starting_ct']\n",
    "    start_ct = selected_row[f'team_{starting_ct_num}']\n",
    "\n",
    "    tr = ''\n",
    "    if start_ct == selected_row['team_2']:\n",
    "        tr = selected_row['team_1']\n",
    "    else:\n",
    "        tr = selected_row['team_2']\n",
    "        \n",
    "    \n",
    "    returnval.append([selected_row['match_id'], selected_row['_map'], selected_row['team_1'], selected_row['team_2'], map_winner, start_ct, tr])\n",
    "    \n",
    "\n",
    "schema = ['partida','mapa', 'equipe1','equipe2','vitorioso','ct','tr']\n",
    "df_mapas = spark.createDataFrame(returnval, schema)\n",
    "display(df_mapas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_in_kudu(df_mapas,'jogos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[partida: string, mapa: string, equipe1: string, equipe2: string, vitorioso: string, ct: string, tr: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = read_from_kudu('jogos')\n",
    "table.createOrReplaceTempView('jogos')\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[match_id: string, _map: string, team_1: string, team_2: string, match_winner: string, best_of: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "condition = [df_limited.match_id == df2_limited.match_id, df_limited._map == df2_limited._map]\n",
    "df_join = df_limited.join(df2_limited,condition,\"inner\").select(df_limited.match_id,df_limited._map, df_limited.team_1, df_limited.team_2,df_limited.match_winner,df2_limited.best_of)\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|team_2|\n",
      "+------+\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|team_1|\n",
      "+------+\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_missing_teams = df_join.select('team_2').distinct().subtract(df_join.select('team_1').distinct())\n",
    "df_teams = df_join.select('team_1').distinct().union(df_missing_teams).withColumnRenamed('team_1','teams')\n",
    "df_join.select('team_2').distinct().exceptAll(df_teams).show()\n",
    "df_join.select('team_1').distinct().exceptAll(df_teams).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[partida: string, mapa: string, equipe1: string, equipe2: string, vitorioso: string, md: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_join = df_join.withColumnRenamed('match_id', 'partida') \\\n",
    "                    .withColumnRenamed('_map', 'mapa') \\\n",
    "                .withColumnRenamed('team_1', 'equipe1') \\\n",
    "                .withColumnRenamed('team_2', 'equipe2') \\\n",
    "                .withColumnRenamed('match_winner', 'vitorioso') \\\n",
    "                .withColumnRenamed('best_of', 'md')\n",
    "display(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_in_kudu(df_join,'proc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[partida: string, mapa: string, equipe1: string, equipe2: string, vitorioso: string, md: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teste = read_from_kudu('proc')\n",
    "teste.createOrReplaceTempView('proc')\n",
    "display(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[equipe: string, jogos: bigint, vitorias: bigint, derrotas: bigint, md1: bigint, md2: bigint, md3: bigint, md5: bigint, jmd1: bigint, jmd2: bigint, jmd3: bigint, jmd5: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_collect = df_teams.collect()\n",
    "data = []\n",
    "for i in range(df_teams.count()):\n",
    "    selected_row = df_collect[i]\n",
    "    current_team = selected_row['teams']\n",
    "    # print(current_team)\n",
    "    \n",
    "    num_jogos1 = spark.sql(f'SELECT COUNT(*) as cnt FROM (SELECT DISTINCT partida FROM proc WHERE equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\") a')\n",
    "    total_jogos = num_jogos1.collect()[0][0]\n",
    "\n",
    "    vitorias = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE (equipe1 = \"{current_team}\" and vitorioso = \"1\") or (equipe2 = \"{current_team}\" and vitorioso = \"2\" ) ) a').collect()[0][0] \n",
    "    derrotas = total_jogos - vitorias\n",
    "\n",
    "    jmd5 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"5\" and (equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\")) a').collect()[0][0]\n",
    "    md5 = 0\n",
    "    if jmd5 > 0:\n",
    "        md5 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"5\" and ((equipe1 = \"{current_team}\"  and vitorioso = \"1\")  or (equipe2 = \"{current_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "\n",
    "    jmd3 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"3\" and (equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\")) a').collect()[0][0]\n",
    "    md3 = 0\n",
    "    if jmd3 > 0:\n",
    "        md3 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"3\" and ((equipe1 = \"{current_team}\" and vitorioso = \"1\") or (equipe2 = \"{current_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "\n",
    "    jmd2 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"2\" and (equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\")) a').collect()[0][0]\n",
    "    md2 = 0\n",
    "    if jmd2 > 0:\n",
    "        md2 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"2\" and ((equipe1 = \"{current_team}\" and vitorioso = \"1\") or (equipe2 = \"{current_team}\" and vitorioso = \"2\") ) ) a').collect()[0][0]\n",
    "\n",
    "    jmd1 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"1\" and (equipe1 = \"{current_team}\" or equipe2 = \"{current_team}\")) a').collect()[0][0]\n",
    "    md1 = 0\n",
    "    if jmd1 > 0:    \n",
    "        md1 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"1\" and  ((equipe1 = \"{current_team}\"  and vitorioso = \"1\") or (equipe2 = \"{current_team}\" and vitorioso = \"2\"))) a').collect()[0][0]\n",
    "        \n",
    "\n",
    "    data.append([current_team,total_jogos,vitorias,derrotas, md1, md2, md3, md5,jmd1, jmd2, jmd3,jmd5])\n",
    "\n",
    "schema = ['equipe','jogos', 'vitorias', 'derrotas','md1', 'md2', 'md3', 'md5','jmd1', 'jmd2', 'jmd3','jmd5']\n",
    "df_equipes = spark.createDataFrame(data, schema)\n",
    "display(df_equipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- equipe: string (nullable = true)\n",
      " |-- jogos: decimal(8,5) (nullable = true)\n",
      " |-- vitorias: decimal(8,5) (nullable = true)\n",
      " |-- derrotas: decimal(8,5) (nullable = true)\n",
      " |-- md1: decimal(8,5) (nullable = true)\n",
      " |-- md2: decimal(8,5) (nullable = true)\n",
      " |-- md3: decimal(8,5) (nullable = true)\n",
      " |-- md5: decimal(8,5) (nullable = true)\n",
      " |-- jmd1: decimal(8,5) (nullable = true)\n",
      " |-- jmd2: decimal(8,5) (nullable = true)\n",
      " |-- jmd3: decimal(8,5) (nullable = true)\n",
      " |-- jmd5: decimal(8,5) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_equipes = df_equipes.withColumn('jogos', df_equipes.jogos.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('vitorias', df_equipes.vitorias.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('derrotas', df_equipes.derrotas.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('md1', df_equipes.md1.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('md2', df_equipes.md2.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('md3', df_equipes.md3.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('md5', df_equipes.md5.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('jmd1', df_equipes.jmd1.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('jmd2', df_equipes.jmd2.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('jmd3', df_equipes.jmd3.cast(DecimalType(8, 5))) \\\n",
    "    .withColumn('jmd5', df_equipes.jmd5.cast(DecimalType(8, 5)))\n",
    "df_equipes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_in_kudu(df_equipes,'equipes')\n",
    "equipes = read_from_kudu('equipes')\n",
    "equipes.createOrReplaceTempView('equipes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- equipe: string (nullable = false)\n",
      " |-- jogos: decimal(8,5) (nullable = true)\n",
      " |-- vitorias: decimal(8,5) (nullable = true)\n",
      " |-- derrotas: decimal(8,5) (nullable = true)\n",
      " |-- md1: decimal(8,5) (nullable = true)\n",
      " |-- md2: decimal(8,5) (nullable = true)\n",
      " |-- md3: decimal(8,5) (nullable = true)\n",
      " |-- md5: decimal(8,5) (nullable = true)\n",
      " |-- jmd1: decimal(8,5) (nullable = true)\n",
      " |-- jmd2: decimal(8,5) (nullable = true)\n",
      " |-- jmd3: decimal(8,5) (nullable = true)\n",
      " |-- jmd5: decimal(8,5) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equipes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j Data Pre Processing\n",
    "This step will use the loaded data in Apache Kudu to pre process specific team win rates, and insert that data into de Neo4j database for later use during result prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating all the nodes\n",
    "team_node_schema = StructType([StructField(\"name\", StringType(), False),\\\n",
    "                     StructField(\"mostPicked\", StringType(), True),\\\n",
    "                     StructField(\"mostBanned\", StringType(), True),\\\n",
    "                     StructField(\"mostWon\", StringType(), True),\\\n",
    "                     StructField(\"mostLost\", StringType(), True)])\n",
    "\n",
    "list_team_name = spark.sql(\"SELECT DISTINCT equipe FROM equipes\")\n",
    "\n",
    "rows = [Row(name=row[\"equipe\"], mostPicked=None,mostBanned=None,mostWon=None,mostLost=None) for row in list_team_name.collect()]\n",
    "\n",
    "df = spark.createDataFrame(rows,schema=team_node_schema)\n",
    "df.write.format(\"org.neo4j.spark.DataSource\")\\\n",
    " .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    " .option(\"authentication.type\", \"none\")\\\n",
    " .option(\"labels\", \":Team\")\\\n",
    " .mode(\"Append\")\\\n",
    " .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating relationships\n",
    "list_map_name = spark.sql(\"SELECT DISTINCT mapa FROM proc\")\n",
    "for x in range(df_teams.count()):\n",
    "    origin_row = df_collect[x]\n",
    "    origin_team = selected_row['teams']   \n",
    "    for i in range(df_teams.count()):\n",
    "        target_row = df_collect[i]\n",
    "        target_team = selected_row['teams']\n",
    "        \n",
    "        if origin_team == target_team:\n",
    "            continue\n",
    "            \n",
    "        jmd5 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"5\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\") or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\")) a').collect()[0][0]\n",
    "        if jmd5 > 0:    \n",
    "            md5 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"5\" and ((equipe1 = \"{current_team}\" and equipe2 = \"{target_team}\"  and vitorioso = \"1\")  or (equipe1 = \"{target_team}\" and equipe2 = \"{current_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "    \n",
    "        jmd3 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"3\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\") or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\")) a').collect()[0][0]\n",
    "        if jmd3 > 0:\n",
    "            md3 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"3\" and ((equipe1 = \"{current_team}\" and equipe2 = \"{target_team}\"  and vitorioso = \"1\")  or (equipe1 = \"{target_team}\" and equipe2 = \"{current_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "    \n",
    "        jmd2 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"2\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\") or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\")) a').collect()[0][0]\n",
    "        if jmd2 > 0:\n",
    "            md2 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"2\" and ((equipe1 = \"{current_team}\" and equipe2 = \"{target_team}\"  and vitorioso = \"1\")  or (equipe1 = \"{target_team}\" and equipe2 = \"{current_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "    \n",
    "        jmd1 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"1\" and ((equipe1 = \"{origin_team}\" and equipe2 = \"{target_team}\") or (equipe1 = \"{target_team}\" and equipe2 = \"{origin_team}\")) a').collect()[0][0]\n",
    "        if jmd1 > 0:    \n",
    "            md1 = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT DISTINCT partida FROM proc WHERE md = \"1\" and ((equipe1 = \"{current_team}\" and equipe2 = \"{target_team}\"  and vitorioso = \"1\")  or (equipe1 = \"{target_team}\" and equipe2 = \"{current_team}\" and vitorioso = \"2\")) ) a').collect()[0][0]\n",
    "\n",
    "        if jmd5 == 0 and jmd3 == 0 and jmd2 == 0 and jmd1 == 0:\n",
    "            continue\n",
    "        \n",
    "        txmd5 = 0\n",
    "        if jmd5 != 0:\n",
    "            txmd5 = md5/jmd5\n",
    "        txmd3 = 0\n",
    "        if jmd3 != 0:\n",
    "            txmd3 = md3/jmd3\n",
    "        txmd2 = 0\n",
    "        if jmd2 != 0:\n",
    "            txmd2 = md2/jmd2\n",
    "        txmd1 = 0\n",
    "        if jmd1 != 0:\n",
    "            txmd5 = md1/jmd1\n",
    "        \n",
    "        relationship_type = 'played'\n",
    "        relationship_properties = {\n",
    "            'bo5': txmd5,\n",
    "            'bo3': txmd3,\n",
    "            'bo2': txmd2,\n",
    "            'bo1': txmd1\n",
    "        }\n",
    "        relationship_df = spark.createDataFrame(\n",
    "             [(origin_team, target_team, relationship_type, relationship_properties)],\n",
    "             ['src', 'dst', 'relationship_type', 'relationship_properties']\n",
    "         )\n",
    "        relationship_df.write \\\n",
    "             .format('org.neo4j.spark.DataSource') \\\n",
    "             .option('url', 'bolt://neo4j:7687') \\\n",
    "             .option(\"authentication.type\", \"none\")\\\n",
    "             .option('relationship.save.strategy', 'keys') \\\n",
    "             .option('relationship.source.labels', ':Team') \\\n",
    "             .option('relationship.target.labels', ':Team') \\\n",
    "             .option('relationship.source.nodes.map', 'name') \\\n",
    "             .option('relationship.target.nodes.map', 'name') \\\n",
    "             .option('relationship', relationship_type) \\\n",
    "             .mode('append') \\\n",
    "             .save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_type = 'played'\n",
    "relationship_properties = {\n",
    " 'bo5': 0.4,\n",
    " 'bo3': 0.80,\n",
    " 'bo2': 0,\n",
    " 'bo1': 0.2\n",
    "}\n",
    "relationship_df = spark.createDataFrame(\n",
    " [(\"fnatic\", \"navi\", relationship_type, relationship_properties)],\n",
    " ['src', 'dst', 'relationship_type', 'relationship_properties']\n",
    ")\n",
    "relationship_df.write \\\n",
    " .format('org.neo4j.spark.DataSource') \\\n",
    " .option('url', 'bolt://neo4j:7687') \\\n",
    " .option(\"authentication.type\", \"none\")\\\n",
    " .option('relationship.save.strategy', 'keys') \\\n",
    " .option('relationship.source.labels', ':Team') \\\n",
    " .option('relationship.target.labels', ':Team') \\\n",
    " .option('relationship.source.nodes.map', 'name') \\\n",
    " .option('relationship.target.nodes.map', 'name') \\\n",
    " .option('relationship', relationship_type) \\\n",
    " .mode('append') \\\n",
    " .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameReader' object has no attribute 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df_teste \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.neo4j.spark.DataSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbolt://neo4j:7687\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrorIfExists\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthentication.type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnavi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m AND n2.name = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfnatic\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m RETURN r.winrate)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      7\u001b[0m display(df_teste)\n\u001b[1;32m      8\u001b[0m df_teste\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameReader' object has no attribute 'mode'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 53750)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_teste = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\",\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = 'navi' AND n2.name = 'fnatic' RETURN r.winrate)\")\\\n",
    "        .load()\n",
    "display(df_teste)\n",
    "df_teste.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[<id>: bigint, <labels>: array<string>, name: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----------+\n",
      "|<id>|<labels>|name      |\n",
      "+----+--------+----------+\n",
      "|0   |[Team]  |Lowkey    |\n",
      "|1   |[Team]  |eRa       |\n",
      "|2   |[Team]  |AlienTech |\n",
      "|3   |[Team]  |2990      |\n",
      "|4   |[Team]  |PANTHERS  |\n",
      "|5   |[Team]  |SKDC      |\n",
      "|6   |[Team]  |xTc       |\n",
      "|7   |[Team]  |Windigo   |\n",
      "|8   |[Team]  |ex-MC     |\n",
      "|9   |[Team]  |Goliath   |\n",
      "|10  |[Team]  |Emprox    |\n",
      "|11  |[Team]  |imperial  |\n",
      "|12  |[Team]  |Impossible|\n",
      "|13  |[Team]  |District 9|\n",
      "|14  |[Team]  |The Prime |\n",
      "|15  |[Team]  |BLITZKRIEG|\n",
      "|16  |[Team]  |OneThree  |\n",
      "|17  |[Team]  |Rekt      |\n",
      "|18  |[Team]  |8EASY     |\n",
      "|19  |[Team]  |Alma      |\n",
      "+----+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"labels\", \"Team\")\\\n",
    "        .load()\n",
    "display(df)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Prediction\n",
    "This is the main step to be used in result prediction, it will load data from Neo4j, while simultaneously processing raw generic data present in Apache Kudu. Results from both databases will then be fed into an algorithm that predicts the winner of the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalMapWR(team, map):\n",
    "    games = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE mapa = \"{map}\" and equipe1 = \"{team}\" or equipe2 = \"{team}\")').collect()[0][0]\n",
    "    if games == 0:\n",
    "            return None\n",
    "    wins = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and mapa = \"{map}\")').collect()[0][0]\n",
    "    return wins/games\n",
    "\n",
    "def generalMapSideWR(team, map, side):\n",
    "    if side == 'TR': \n",
    "        games = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE mapa = \"{map}\" and (equipe1 = \"{team}\" or equipe2 = \"{team}\") and tr = \"{team}\")').collect()[0][0]\n",
    "        if games == 0:\n",
    "                return None\n",
    "        wins = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and mapa = \"{map}\" and tr = \"{team}\")').collect()[0][0]\n",
    "        return wins/games\n",
    "    if side == 'CT':\n",
    "        games = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE mapa = \"{map}\" and (equipe1 = \"{team}\" or equipe2 = \"{team}\") and ct = \"{team}\")').collect()[0][0]\n",
    "        if games == 0:\n",
    "                return None\n",
    "        wins = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and mapa = \"{map}\" and ct = \"{team}\")').collect()[0][0]\n",
    "        return wins/games\n",
    "\n",
    "def generalSideWR(team, side):\n",
    "    if side == 'TR':\n",
    "        games = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida, mapa FROM jogos WHERE tr = \"{team}\")').collect()[0][0]\n",
    "        if games == 0:\n",
    "            return None\n",
    "        wins = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and tr = \"{team}\")').collect()[0][0]\n",
    "        return wins/games\n",
    "    if side == 'CT':\n",
    "        games = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida, mapa FROM jogos WHERE ct = \"{team}\")').collect()[0][0]\n",
    "        if games == 0:\n",
    "            return None\n",
    "        wins = spark.sql(f'SELECT COUNT(*) as jogos FROM (SELECT partida,mapa FROM jogos WHERE vitorioso = \"{team}\" and ct = \"{team}\")').collect()[0][0]\n",
    "        return wins/games\n",
    "\n",
    "def generalTeamWR(team):\n",
    "    games = spark.sql(f'SELECT jogos FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return None\n",
    "    wins = spark.sql(f'SELECT vitorias FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return wins/games\n",
    "\n",
    "def generalBO1WR(team):\n",
    "    games = spark.sql(f'SELECT jmd1 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return None\n",
    "    wins = spark.sql(f'SELECT md1 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return wins/games\n",
    "\n",
    "def generalBO2WR(team):\n",
    "    games = spark.sql(f'SELECT jmd2 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return None\n",
    "    wins = spark.sql(f'SELECT md2 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return wins/games\n",
    "\n",
    "def generalBO3WR(team):\n",
    "    games = spark.sql(f'SELECT jmd3 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return None\n",
    "    wins = spark.sql(f'SELECT md3 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return wins/games\n",
    "\n",
    "def generalBO5WR(team):\n",
    "    games = spark.sql(f'SELECT jmd5 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    if games == 0:\n",
    "        return None\n",
    "    wins = spark.sql(f'SELECT md5 FROM equipes WHERE equipe = \"{team}\"').collect()[0][0]\n",
    "    return wins/games\n",
    "\n",
    "def specificTeamToTeamWR(team1, team2):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = {team1} AND n2.name = {team2} RETURN r.winrate)\")\\\n",
    "        .load()\n",
    "    display(df)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "def specificTeamToTeamMapWR(team1, team2, map):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = {team1} AND n2.name = {team2} RETURN r.{map}-winrate)\")\\\n",
    "        .load()\n",
    "    display(df)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "def specificTeamToTeamBO1WR(team1, team2, map):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = {team1} AND n2.name = {team2} RETURN r.bo1winrate)\")\\\n",
    "        .load()\n",
    "    display(df)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "def specificTeamToTeamBO2WR(team1, team2, map):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = {team1} AND n2.name = {team2} RETURN r.bo2winrate)\")\\\n",
    "        .load()\n",
    "    display(df)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "def specificTeamToTeamBO3WR(team1, team2, map):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = {team1} AND n2.name = {team2} RETURN r.bo3winrate)\")\\\n",
    "        .load()\n",
    "    display(df)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "def specificTeamToTeamBO5WR(team1, team2, map):\n",
    "    df = spark.read.format(\"org.neo4j.spark.DataSource\")\\\n",
    "        .option(\"url\", \"bolt://neo4j:7687\")\\\n",
    "        .option(\"authentication.type\", \"none\")\\\n",
    "        .option(\"query\", f\"MATCH (n1:Team)-[r:played]->(n2:Team) WHERE n1.name = {team1} AND n2.name = {team2} RETURN r.bo5winrate)\")\\\n",
    "        .load()\n",
    "    display(df)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "def predict(team1 = \"\", side1 = \"\", team2 = \"\", side2 = \"\", map = \"\"):\n",
    "    pass\n",
    "    # Decide what functions to call based on the existence of the arguments\n",
    "# TODO - Define functions that will interpret the match input and call multiple queries to fetch the win rates\n",
    "# TODO - Define the algorithm that takes the win rates and outputs final prediction\n",
    "# TODO - Define an interface for the user to call the interpreter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipes.show()\n",
    "print(generalMapSideWR(\"100 Thieves\", \"Mirage\", \"CT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Queries\n",
    "This section is used to call other queries not linked to result prediction like objective queries or player statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low priority TODO \n",
    "# Define functions to execute objective queries in Neo4j\n",
    "# Define functions to execute generic statistics queries in Apache Kudu\n",
    "# Define interface to call above functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
